{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   80 fps= 14 q=-0.0 size=N/A time=00:20:00.00 bitrate=N/A speed= 216x    \r"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import boto3\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Step 1: Transcribe Video\n",
    "def transcribe_video(video_path):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-tiny\",device=device)\n",
    "    transcription = transcriber(video_path, return_timestamps=\"word\")\n",
    "    return transcription\n",
    "\n",
    "# Step 2: Extract Frames and Pair with Dialogue\n",
    "def extract_frames(video_path, interval):\n",
    "\n",
    "    output_dir = os.path.splitext(video_path)[0]\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Use ffmpeg to extract frames\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(video_path)\n",
    "        .filter('fps', fps=1/interval)\n",
    "        .output(f'{output_dir}/frame_%04d.png')\n",
    "        .run()\n",
    "    )\n",
    "    \n",
    "    # Collect the frame file paths\n",
    "    frame_files = sorted([os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith('.png')])\n",
    "    # create a dictionary of frame file paths and their corresponding timestamps according to interval and video length\n",
    "\n",
    "    frames_and_intervals = []\n",
    "    interval_start = 0\n",
    "    for i, frame in enumerate(frame_files):\n",
    "        single_frame = {\n",
    "            \"frame_path\": frame,\n",
    "            \"timestamp\": (interval_start, interval_start + (interval))\n",
    "        }\n",
    "        frames_and_intervals.append(single_frame)\n",
    "        interval_start+=interval\n",
    "    \n",
    "    # get vieo duration and make the last frame the same as the duration\n",
    "    video_duration = ffmpeg.probe(video_path)['format']['duration']\n",
    "    final_frame_start_time = frames_and_intervals[-1][\"timestamp\"][0]\n",
    "    frames_and_intervals[-1][\"timestamp\"] = (final_frame_start_time, float(video_duration))\n",
    "    return frames_and_intervals\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/pinecone-claude/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "WhisperModel is using WhisperSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "# try the  transcription on the file in the folder\n",
    "test_file = \"mlsearch_webinar.mp4\"\n",
    "\n",
    "#without timestamps about 1.5 minutes for 45min of video\n",
    "# 15s to pull the frames. getting the stuff in the right format is fast, basically negligible\n",
    "#with timestamps, about same time\n",
    "# we can use speculative decoding to speed up the process by 2.2x ltr, when we have a good pipline going\n",
    "\n",
    "transcription = transcribe_video(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.0.2 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 15.0.0 (clang-1500.3.9.4)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.0.2 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59.  8.100 / 59.  8.100\n",
      "  libavcodec     61.  3.100 / 61.  3.100\n",
      "  libavformat    61.  1.100 / 61.  1.100\n",
      "  libavdevice    61.  1.100 / 61.  1.100\n",
      "  libavfilter    10.  1.100 / 10.  1.100\n",
      "  libswscale      8.  1.100 /  8.  1.100\n",
      "  libswresample   5.  1.100 /  5.  1.100\n",
      "  libpostproc    58.  1.100 / 58.  1.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'mlsearch_webinar.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    creation_time   : 2024-09-07T05:52:15.000000Z\n",
      "    encoder         : Google\n",
      "  Duration: 00:53:43.77, start: 0.000000, bitrate: 302 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, smpte170m/bt470bg/bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], 171 kb/s, 25 fps, 25 tbr, 12800 tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-09-07T05:52:15.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 09/06/2024.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x2](eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 127 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2024-09-07T05:52:15.000000Z\n",
      "        handler_name    : ISO Media file produced by Google Inc. Created on: 09/06/2024.\n",
      "        vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 (h264) -> fps:default\n",
      "  fps:default -> Stream #0:0 (png)\n",
      "Press [q] to stop, [?] for help\n",
      "[swscaler @ 0x111060000] [swscaler @ 0x1200f8000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x111060000] [swscaler @ 0x120408000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x111060000] [swscaler @ 0x120418000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x111060000] [swscaler @ 0x1206c0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x111060000] [swscaler @ 0x1206d0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x111060000] [swscaler @ 0x1206e0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x111060000] [swscaler @ 0x1206f0000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x111060000] [swscaler @ 0x120700000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "[swscaler @ 0x111060000] [swscaler @ 0x120710000] No accelerated colorspace conversion found from yuv420p to rgb24.\n",
      "Output #0, image2, to 'mlsearch_webinar/frame_%04d.png':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: isommp42\n",
      "    encoder         : Lavf61.1.100\n",
      "  Stream #0:0: Video: png, rgb24(pc, gbr/bt470bg/bt709, progressive), 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 0.07 fps, 0.07 tbn\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.3.100 png\n",
      "[out#0/image2 @ 0x12c704cc0] video:62353KiB audio:0KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: unknown\n",
      "frame=  215 fps=9.0 q=-0.0 Lsize=N/A time=00:53:45.00 bitrate=N/A speed= 135x    \n",
      "[out#0/image2 @ 0x11d606100] video:62353KiB audio:0KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: unknown\n",
      "frame=  215 fps=9.1 q=-0.0 Lsize=N/A time=00:53:45.00 bitrate=N/A speed= 137x    \n"
     ]
    }
   ],
   "source": [
    "# get video frames\n",
    "INTERVAL=15\n",
    "frames = extract_frames(test_file, 15)\n",
    "# get dialogue image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'frame_path': 'mlsearch_webinar/frame_0001.png', 'timestamp': (0, 15)},\n",
       " {'frame_path': 'mlsearch_webinar/frame_0002.png', 'timestamp': (15, 30)},\n",
       " {'frame_path': 'mlsearch_webinar/frame_0003.png', 'timestamp': (30, 45)}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_words_to_frames(transcription, frames, interval=INTERVAL):\n",
    "    # given transcription word chunks and frames that occur w.r.t interval, assign words to frames\n",
    "    # Transcription will be on word level, so for each frame, we find the words that occur in the time interval of the frame\n",
    "    # and assign them to the frame\n",
    "\n",
    "    frames_and_words = []\n",
    "\n",
    "    for f in frames:\n",
    "        frame_start, frame_end = f[\"timestamp\"][0], f[\"timestamp\"][1]\n",
    "        \n",
    "        # filter for words that fall in frame\n",
    "        words_in_frame = list(filter(lambda w: w['timestamp'][0] > frame_start and w['timestamp'][1] < frame_end, transcription[\"chunks\"]))\n",
    "        words = [w['text'] for w in words_in_frame]\n",
    "        words= \"\".join(words)\n",
    "        single_frame = {\n",
    "            \"frame_path\": f[\"frame_path\"],\n",
    "            \"words\": words,\n",
    "            \"timestamp\": f[\"timestamp\"]\n",
    "        }\n",
    "        frames_and_words.append(single_frame)\n",
    "    return frames_and_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align the new frames with the dialogue, so we have a list of dictionaries\n",
    "# each dictionary will be the frame filepath, the dialogue, and the timestamp in a metadata field\n",
    "\n",
    "\n",
    "def align_frames_with_dialogue(frames, transcription):\n",
    "    #Pinecone for t\n",
    "    dialogue_frames = []\n",
    "    for frame, chunk in zip(frames, transcription[\"chunks\"]):\n",
    "        dialogue_frames.append({\n",
    "            \"frame\": frame,\n",
    "            \"dialogue\": chunk[\"text\"],\n",
    "            \"metadata\": {\n",
    "                \"timestamp\": chunk[\"timestamp\"]\n",
    "            }\n",
    "        })\n",
    "    return dialogue_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_and_words = assign_words_to_frames(transcription, frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" All right, welcome everybody. Thanks for coming today. This webinar is going to be on the magic of multi-lingual search. I'm really excited to help everybody learn about using and applying multi-lingual search to whatever you might\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_and_words[0][\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write frames and words out as json\n",
    "\n",
    "with open(\"frames_and_words.json\", \"w\") as f:\n",
    "    json.dump(frames_and_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinecone-claude",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
