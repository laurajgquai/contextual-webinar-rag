" All right, welcome everybody. Thanks for coming today. This webinar is going to be on the magic of multi-lingual search. I'm really excited to help everybody learn about using and applying multi-lingual search to whatever you might be working on. This is me, I'm Arjun. I'm a developer advocate at PineCone and I love making technical content and blog posts and all that good stuff. The thing I love the most is learning about something that's really interesting or challenging and making it more digestible for other people to use and think about. And recently, I'm in thinking really hard about what multilingual search is, how it works, how we can use it so on and so forth. Because I think that it's very easy to take it as a given, much like how we take universal translation as a given. But there's a lot to unpack there. And I hope that during this webinar we can kind of uncover how this works, how we can apply it for vector search, so on and so forth. And just so everybody knows we'll be recording the talk so you can rewatch this much later. And there's an open Q&A so if you have any questions during the talk we have set aside about 20 minutes at the end of this in order to answer those. So please throw them in there as you get them. What we'll be going over today is why we need multi-lingual search, specifically multi-lingual semantic search. We're going to have a mini crash course going from very little knowledge about what search and vectors are and so on and so forth. on so forth and ending at what multilingual semantic search is and how you can use it. We're going to learn a bit about vector embeddings. We're going to learn about how multilingual large language models are made and why they're able to represent concepts across languages. Learn about how to take a multilingual large language model and turn it into an embedding model that's great for multilingual semantic search through the lens of multilingual E5 large and pine cone serverless and inference. And finally, I'll show you a demo that applies cross-lingual and model-lingual search to a language learning problem. So not only will you learn about these things, but you'll also see how to use them in a semantic search application. All right, having said that, let's dive right in. So let's say you want to learn Spanish because you're an English speaker and you're trying to pick up the second language. Your class assignment is to write about your weekend trip that you took to a local park. There's this website called TOTOBA that you come across, which is basically a really good search engine for sentence translations. Contributors can go on the website, type in a sentence, and if you speak another language, you can type in the translation of that sentence in order to help other learners find sentence translations. The nice property here is that if there are enough pairs across languages, you can start to connect things across those languages and learn about how people say things in all sorts of ways. And your teacher is kind of tasked to use this website to help you create your response. But there's one big problem. The problem is that Tatoba only supports keyword search, which means if you search for a word like park, you're going to get sentences that are related to parking, which is completely different than going to a park. So there's an overloading on the keywords here. What would really be nice? And of course, if you look for exact sentence translations, if you look for exact English sentences and look for Spanish census back, you're just going to get exact translations of that sentence in different variations, which might not be as interesting to you if you're trying to write a paragraph or an essay about what you did. You might want lots of variation in what you're doing. It would be really great if instead of typing in a concept or a set of words and getting back just the keyword overlap of all the sentences that are on the website, we would actually find not only the literal sentence translation that could exist on the website, but any similar and relevant senses that are along the lines of whatever concept we're interested in, which is going to a part for a week and trip, in addition to that translation. Keyword Search can't help us do that because it's not easy to extract that concept out using just the words that are there. This is a multilingual semantic search problem. We're trying to search over a corpus that exists in multiple languages, using multiple languages, and We're specifically looking for a comparison with respect to meaning rather than keyword overlap. Now when we talk about multilingual semantic search, we're not only talking about cross-lingual search, which is not as common of an application, but we're also talking about scaling semantic search across languages. So if you have a bunch of customers that speak in English and need to access English documents, you can reuse the same pipeline for customers who speak Spanish and are trying to search for Spanish documents. That's a benefit of doing multilingual semantic search in the way that we're going to show today. And of course offering that cross-lingual approach could be useful in certain specific applications. For example, if you're trying to analyze reviews and you have reviews coming in from loads of countries all over the world, you might want to do some sort of topical analysis on those reviews or cluster them in vector space. Multilingual Semitic Search can help you do that. If you want to make your documentation more accessible to people who speak English as a second language, You could enable multilingual semantic search to not only help English speakers figure out what they need, but also to help anybody typing in any language to find the documentation that you're interested in. Also, the application that we're going to be interested in is going to be education, which is helping people figure out how to find these relevant sentences. The other things that you can do is just scale your existing multilingual So, as a SMADX search pipelines, which is what I've said prior, or you can learn about companies around the world if you're in some sort of financial domain by enabling this type of SMADX search. So, that's all well and good. But how do we actually achieve and build this type of multi-lingual SMADX search system? It sounds very complicated. Like, how can we translate all these languages or create these concepts across them and actually index the information? So, we break the problem down into two parts. The first is we need a way to represent information across languages. And the second is we need a way to efficiently search over this information, especially as the corpus scales. So we're going to solve the first problem by understanding multilingual embedding large language models, and we'll solve the second problem by using Panko and Panko's vector database. Now before we go into that, it's really important to kind of level set on what we know about about how large language models work, and how we can use large language models to get embeddings. In the AI space, there are lots of vocabulary, lots of words that are thrown around. It can get very intimidating to kind of weigh through all of that in order to build the thing that you're actually interested in building. And at the end of this talk, I hope to kind of defog all of the confusion that can exist around these terms. And to give you a nice way of thinking about what you need to know in order to start building such a system. And it turns out it's not actually that much. So we're going to start right at the beginning. Water vectors and water vector embeddings as a refresher. Vectors, I like to think about them as ways of representing non mathematical objects into fixed-sized lists of numbers. So you can see on my diagram on the right, we have a sentence, which is I went to the park over the weekend. We toss it at a vector embedding model, whatever that may be. And it outputs this fixed-sized list of numbers, usually floating point numbers. And usually it's fixed-sized because we want to compress information into the same dimension for reasons that are important later. Now, the nice thing about turning non mathematical things and some mathematical things is that you can apply mathematical rules to those things. For example, if you have a list of numbers and you map that to a coordinate space, there are certain properties about those vectors that exist in coordinate space that you can do comparison on in order to understand their relationship to one another. So it would be super cool if we had a magic box that could take us from whatever senses we're interested in for example a question like when were the Normans in Normandy and help us cluster that representation close to responses that would be relevant. For example, this longer context that explains when and where the Normans were in Normandy. So that's nice because if we can measure similarness and content and relevance with respect to where these concepts exist in vector space, Then we can automate the process of searching for things that are similar, without having to worry about thinking of ways of handling what these concepts are and what words people are using, what they're trying to communicate. We can just directly transform those things into the mathematical space we're interested in. We don't have to think about how we have to do additional preprocessing to get the information out. We can just take the representation as it is and its transformation into whatever space we're looking at. We're getting a little ahead of ourselves because I haven't necessarily justified why this is possible for text in the first place When I try to explain to people what vector embeddings are I really like to start from the Distribution hypothesis, which is this idea from computational linguistics Basically the theory goes that if you have a bunch of words in a giant corpus think of all a literature in the world or all of the Wikipedia Articles on Wikipedia If you were to look at words specifically, specific single words or sets of words, like biograms or trigograms, or what have you, how they cluster in vector space should be relative to how they are croquering inside those texts. So if you think about the word moon and the word space, you expect to see those words closer to one another inside text than you would relative to other unrelated words like oceans or fish or something like that. So there's something interesting about this property of co-occurrence that happens in language that we should be able to take advantage of in order to learn text representations of those words. For example, maybe there's something we can learn about the word space by looking at the word that surround the word space in the wild. That's the basic idea. And so when we think about how we're learning these models for language, We are really thinking about some heuristic or some way of understanding how these words sentences, documents are distributed across the text that we're working with. Hence, language models, right? Model, language, language models, because we're trying to understand this property. I want you to keep in mind co-occurrence and this idea of words and concepts come out occurring together as we progress through the rest of this webinar, but that kind of helps me develop a more theoretical understanding of what's kind of going on. OK, so LLLM's like, what are they? what do you need to know? I think the most important thing to take away here is going to be that large language models are basically extremely powerful neural networks that have been trained on billions of text data points, which have learned this ability to abstract concept and meeting from different senses based on certain properties of that text corpus. And what large language models allow us to do is take arbitrary words, document sentences, and turn them into vector embeddings in such a manner that they encode the meaning of contained inside those word senses or documents in those vector embeddings such that we can do computation on those vector embeddings to compare the meaning, the relevance, or whatever other properties were interested in the content of the text for the type of applications that we're building. That's why large language models are super useful for us. So the first large language model we're going to talk about is X-Lem Roberta because it was the first biggest example of scaling up training of dis-type of unsupervised co-occurrence, LLM approach for creating a really good multi-lingual embedding model. So a lot of the models that were multi-lingual trained before this point in about 2020 weren't at, they were pretty large but they weren't trained at the entire internet's worth of scale. And basically the contribution of this paper is that if you scale up this effort and you have some clever ways of handling all the different types of languages that exist in the world, you can actually build a really good multilingual cross-lingual embedding representation of the languages inside the score list, just by working on the scaling up. The key contributions and the key innovations that occur from this paper that allow us to create this representation across languages are going to be this training data scale up, language, agnostic tokenization, and this training objective of mass language modeling in order to get multilingual embeddings. We're going to go over what each of these are really quickly, just to get us a good understanding of why this model works as well as it does. The dataset that excellent Roberta uses is massive. It covers 88 different languages. You can see that this is a graph from the paper, and on the y-axis is powers of 10, so it's a log scale in gigabytes. And basically, the excellent Roberta paper is using both halves of the data set that are being described here, common crawl, which is basically a huge, huge portion, if not all of the internet and Wikipedia. And the x-axis here is going to be all the languages that are within the corpus. One thing I want you to keep in mind is that a lot of the approach before this paper was kind of trying to create models for specific languages or trying to indicate one specific language was in use or inside the corpus. And this paper kind of does away with that. There's no indication of what languages being trained on at an even point. and there's no separation in the tokenization of those languages. And that's actually kind of nice because languages exist in families. And so if you have low resource languages like the ones that exist at the right side of the graph, which might not have as much data as the ones in English or Russian or all these other languages, you can leverage the fact that these languages have similar behavior to one another in order to get enough data that you need to represent what that language is doing. And that's why training across languages can be really beneficial. Of course, with the specific paper, there is a little bit of an asterisk on that. There is a limit on how well this works as you scale up. But with subsequent papers, we've kind of seen this work pretty good to scale across languages. So the next biggest problem is the one that I find really interesting as a former data scientist, how are they able to handle all these different languages? Language is used, loads of different characters, loads of different concepts. They might be written differently, also on so forth. How are they able to stuff this into the model that's being applied? So what they did was they were able to use a tokenizer in order to break up all of these words that exist across these languages into subboards, which are called tokens. So you can see on the right side of the screen, we have this application where you can pass in a sentence. It breaks the sentence up into how we view it, which is by words. And when we pass it to the tokenizer, which is this magical box, it splits up these things into subboards and then assigns them IDs based on the unique subword that they correlate to. Now this is important because if we took every single vocab word from all the 88 languages, that would be way too many representations for our model to learn. Rather, we want to create a smaller set of subboards that covers as much of the variation in languages as possible and then contextually learn embeddings on those subboards in or to address all of them. And the excellent R paper uses 250k token vocabulary in order to cover all of those languages. And just like how I said prior that some languages exist in language families, so there is a little bit of overlap. That's kind of where the sub tokenizer is able to kind of handle those different things. And this is not intuitive or this is not straightforward to handle across languages. But they were able to kind of address this and or to learn these token representations that scale across them. Second is the learning objective. The big learning objective that excellent Roberto uses is masked language modeling. What this means is that the way we're trying to understand the representations of the concepts that we're working on, It's going to be based on this idea of co-occurrence inside sentence text. So we might have some sentence like I want to play in the park. We apply a mask to the sentence, which basically just randomly drops out portions of the words that exist in the sentence. And we ask the large language model, can you predict the words that should have been here? Just like the idea that I was explaining earlier about the distributional hypothesis and word co-occurrence, maybe there's something useful we can learn about what these words mean in context based on the words that they're surrounded by. And maybe by teaching a large language model to predict the missing words, we can have it learned really good intermediate representations of what those words are going to mean or do. And that's what ends up happening with large language models. Now, for the astute among you, you might realize that this type of model is only going to learn token-level embeddings, which is this idea that you have these subwords and you're learning representations for those subwords. But it's not necessarily intuitive how we go from token-level embeddings to document level embeddings. Like when we have larger queries or things like that, what excellent Roberta does is it actually uses mean tooling. So it averages all the token vectors in order to get some representation. But we'll see that that is an actually good enough for our purposes. What do I mean by that? Why isn't that good enough for our purposes? Well, it turns out that excellent Roberta's really powerful representing tokens, really powerful representing concepts, but it's not that great at producing document level embeddings. It doesn't even produce those directly. You have to apply some transformation in order to get that. What we really want for a good vector search application is an embedding that can handle asymmetries in search. The idea that you might have queries, which could be really short or really long, questions or queries that people are passing to a search engine, and then you'll have documents, which could be longer chunks of information that might represent concepts differently than the queries, well. We also want some sort of vector embedding that can help us deal with relevance and importance in search, in addition to multilingualism and semantic information, which excellent Roberto might not be so great at, because it wasn't trained with these objectives in mind. So what do we do if we want that? Well, this is where the multilingual E5 large model comes in. The cool thing about the multilingual E5 large model, which the technical report for it was released earlier this year, is that it's actually based on the excellent R Roberto model. So it gets a lot of its multilingualism from there, but the research has applied the same three concepts of having really great training data, a solid training process, and a fine tuning mechanism in order to create fantastic document level embeddings for a search within and across languages. And we're going to kind of talk about how multilingual E5 large is able to achieve this. So the biggest reason why multilingual E5 large is able to kind of address this in the first place is at the data set level. When we were working with data in the previous task with excellent R-R-R-R-R-R-R-R-R, we were kind of looking at a synthetic data task, right? where we have some text, we're applying some modification to that text, and we're learning about properties of that text relative to the modification that we're making. The researchers in this paper were like, well, that works really well, and you can scale up that approach, but we're kind of hitting the limits of how well we can learn these search embeddings based on that ascendetic approach. And then on the other hand, you have manually human-labeled data. You might have people who have done queries on some search engine, find the relevant results, and label them. So this is extremely expensive to get. So even though that data is really high quality and we need a lot more of that, that's also really hard to scale up. So what can we change about the training data used for these search models in order to kind of get them to work a lot better in the context that we're interested in? So the researchers set out to scrape a bunch of real-world text pairs, IE, pairs of texts that occur, close together in text vector space in order to take advantage of the fact that if these texts do close together in vector space, that probably or in text space, that probably encodes some sort of relevance information. So there's here are a few categories of the data that they use. They use article and content data from places like Wikipedia, but also on the internet. For example, you might have a title that's just one word or a couple words like origami and you'll have a little passage that kind of corresponds to its relationships to that title. You might have translation pairs that might exist on the internet like on Totova. You might have question answering that exists in different languages and the question and the answer are going to be relevant to one another, especially if the answer is correct, relative to the question. Taking advantage of these natural pairs that exist across the internet helps them create a dataset of relevancy, which is used to change the embeddings of the excellent Roberto model to good search embeddings. The second thing that you need is a really solid pre-training objective. And the researchers introduce something called weekly supervised contrastive pre-training. sounds very scary and very weird, where really all it means is that you're pushing together pairs that should be close together in vector space and you're pushing away pairs of things that should be far away in vector space relative to their relevance and semantic meaning. So you might have a batch of training data that's sent over to the model during training that could include a query and a relevant passage which is represented by the yellow text here, the yellow query and the yellow document. And you might have in batch negatives which are just other randomly sample passages they're not actually relevant to the query, which are represented by this other color here. You pass that to the model, and you look at how the model is clustering the query in the correct passage versus the query in all of their incorrect passages. And you want to push away the, you want to train the model in such a manner so that it pushes away the incorrect passages in vector space and pushes the correct passage together in vector space. And that is allowing us to encode this relevance information and that's important for such processes. The third and final contribution that the researcher should make in order to make multilingual search work really well is not only do you start by having this model and then you train it in this fashion, so it's kind of good at doing that task in general. So you also need to use specific data sets that are designed for retrieval tasks in order for this model to get really good at doing those types of tasks. So on the bottom right is a table from the technical report that describes the different research papers that were used in the different data sets that were used in the fine tuning process. And in my chart on the left, I took the liberty of kind of going into each those data sets and figuring out what broader buckets they kind of fall into. So these data sets cover question answering, duplicate identification, multilingual retrieval. So you might have some question in one language, find it in the same language, or you'll have some question in one language, find it in a different language, natural language inference, which is finding the relationship between two sentences, and just regular English retrieval. The researchers also use other more state-of-the-art techniques like finding the hard negatives for the points that you're interested in, knowledge isolation, and cross encoding in order to really, really solidify the idea of having a good embedding model across languages. And that's also, now we have a way of representing text across languages that is specifically to infer multiple-lingual semantic search. But we have another problem. We've embedded all of our data, and we have tons and tons of vectors. But how do we actually search over those vectors efficiently to find the information that's relevant for us? That's where pine cones vector database kind of comes in. What ends up happening when you have a bunch of vectors Invector space is that you'll have some user query, you'll want to embed it, and then you want to map that user query or measure that user query against all of the documents that exist in your vector database, and you want to find the documents that are closest together in vector space, which is going to be the highest probability of things that are relevant for your user. For example, if you're user asks a question, you want the most relevant answer documents, it kind of pop up and be close together in vector space. Problem is that when you have tons and tons of chunks and tons and tons of documents, this comparison that you have to do against every single document becomes very computationally expensive. And because it's like a database, you need a lot of management type things in order to make it work for your application. And that's what PAN-KON kind of abstracts away. It helps you kind of index this information in a really efficient fashion so that you can find the closest vectors in vector space. And it turns out that once you turn a multi-lingual or multiple things in different languages into vectors, they're just vectors. So it doesn't matter to the vector database, what language your content is in, as long as it's represented in the same vector space. This is where a pine cone inference in pine cone server is come together to make it super easy for us to build this multi-lingual search application. We're able to use pine cone inference, which hosts the multi-lingual E5 large model inside the same SDK as the inference database, which means you can embed your data and upstart your data into your index without having to leave the pine cone universe, which is super nice to use a single API for both purposes. And our demo application is going to the kind of show us how to do that. So here's some example of how you would use the Python inference offering in more to get embeddings for some text that you're passing in. You don't need to think about hosting them model. You don't need to think about tokenizing your data because the tokenizer is located with us. And you're able to embed the data and upstart it within the same workflow. Then after you've embedded an upstarted the data, you can query. You can embed your query and search over your index in order to find the most important data that you want to find back, which is pretty straightforward. not that many additional lines of code. OK, enough of me going on about all of these different concepts. I think it's time for us to actually build this application now. It's talking about where we apply a multilingual semantic search to Toba to learn English in Spanish a little better. So this is what we're going to build in the demo. What we're going to do first is a bunch of data preprocessing in upstirtian steps where we take all of the English and Spanish translation pairs from the Toba dataset. We sub-sample them so that we can kind of get a better way of working through the data set. We split them, we embed them, and upload them to our vector database. So they will exist together and we won't know what sentences or translations of the other. We do this just for instructive reasons just to understand the capability of the vector database to search across languages in addition to pine cone inference supporting those languages. Then we're going to demonstrate how to do model-lingual search where you have one sentence in one language and you search over it in the same language and cross-lingual search where you have one census one language and you search over in the other language and how you can use the same pipeline to support multilingual semantic search for this operation which will be super interesting and important. So I'm going to take a moment to fire up my Jupyter notebook so that we can kind of see what is going on so everyone would give me just a moment to switch tabs here. All right, just a second. Let's see if that looks going on screen. All right, that looks fantastic. All right, this notebook is available in the Panko embedding Panko in examples. Learn section, I'll drop the link in the chat in just a moment. But you can access the notebook and just use it directly and call up to follow along in the examples. So what we're going to do is first install our dependencies. We're going to install PineCone and the datasets library from hugging face. And we're also going to install PineCone notebooks in order to kind of make it easier to do some sort of some of the connections that I was talking about prior. And I've went ahead and installed those already. So I'll just kind of scroll past this. We're going to use PineCone inside collab in order to activate and set our API key, which I've set already. And we'll download and install the Totova dataset. I'm using Helsinki NLP's instantiation of the Totova dataset. It's really big and we're probably not going to be able to search across the whole dataset and a really nice clean fashion. So instead of doing that, what we're going to do is stub sample the dataset. So it's a little easier for us to work with. When we download the data, we can see that it comes in the form of ID and translations. And we have English pairs. For example, the sentence, let's try something. And Spanish pairs, which is going to say intent and most I'll go, which is pretty awesome. Great. Like I said, we probably won't be able to deal with the entire data set of like 200,000, two million plus pairs just for English and Spanish. So we're gonna sub-sample the data. And the way we're gonna do this is we're just gonna look for all strings that have the word park in them. In order to kind of show this instructive example of one word having multiple meetings and how you can use semantic search to kind of bridge the gap in those situations. So all we're doing is looking over all of the pairs on the English side, finding the sentences that have the word park in them and sub-sampling the data so that we're left with about 416 translation pairs that kind of use this idea. And when we look at the first translation pair, we get the sentence, when my brother was young, I often used to take him to the park, which is great. That's relevant for our purposes. We can explore the data a little bit, just by walking over this stuff. And you can see that we are getting sentences that are not necessarily as relevant to the problem that I was describing. So you're not allowed to park your car here. That's not what we want to Let's see what we can do. Next thing we're going to do is set up our index. This is super easy with PineCom Serverless. We're going to create our index name, the dimension, which represents the size of the embeddings that the model is going to output. And we're going to check if the index already exists. And if it does, we're not going to override it. So we can go ahead and run that. Second thing we're going to do is embed our data. One thing I want everyone to kind of come away with is that when you use the multi-lingual E5 large model, we're going to represent queries and passages differently in your data set. So when you're uploading data to your index, in this case, we're uploading the sentences that exist in the data set into the index. We're going to upload them with the input type of passage because we are searching over that data in our vector database. The queries are going to be the strings that we're passing to the vector database that we're trying to find relevant stuff for. This asymmetry is important because you might have really long chunks of documents, which might be encoded differently than the queries with respect to the model in order to represent the concepts that are existing inside those documents. So I have a function here that takes a list of sentences embeds them and returns the embeddings. So we're gonna pull out the translation pairs separately and we're going to embed them and we'll get this embedding list object back which has a property of data and values which is going to be the actual embeddings. The next thing we're gonna do was we're going to batch embed and up our data to the index. It's important to keep in mind that the batch size for our model is 96. So we can embed like 96 different things at the same time. And that we have to keep in mind that we can't pass in more than I believe 57 tokens for each of those embedding. So that's something we'll have to be careful about. Because we're working with such a small corpus, we're just going to embed the data in batches directly without thinking about the rate limits. But if you were to scale up this problem, you would want to make sure that you're attending to the rate limits of the provided by us at Pinecom. So now what we've done is we've taken the text. We have a set of it. We have a patched the language information about the text which will be important for the querying later. And we've attached IDs to the text to make it easier for us to observe. We're going to use the map functionality from the data sets library to make it easier to kind of batch embed all of the sort of stuff. All we're doing is we're mapping over the set of data that I described prior. We're chunking it into batches, and we're passing it to our embedding and upset pipeline in order to get it inside our vector database. That's what this function is going to do. And the handy-dandy sentence mapping operation is going to make it super easy for us to embed those things at the same time. Like I said, if you are going to be doing this on your own and you're uploading more than like 10,000, 20,000 sentence pairs, you might want to think about, implementing some sort of ability to look for rate limiting. And in our documentation, we do have ways of kind of handling this. Cool. So I'm going to let that run a little bit. We're almost done getting our embeddings created and upstarted into the index. Then we have the fun part, which is going to be actually querying that stuff in the vector database. So I'll go back and just take a quick look. All right. It looks like our map operation is done. And we can move on to the second part. And we can see that we get this embedding kind of out. Now we're going to do our embedding function for the senses. So this is the idea that we are going to have queries. So things we're going to ask our vector database that are going to be represented differently using the input type of query. And then we're going to look for all the stuff in vector space that is close to that query in vector space, no matter what language it actually is at. Now in order to kind of be a little more instructive about what is happening here, We're going to add a filter on our query operation to only look for the English sentences and we're going to use an English sentence. Just to demonstrate that this pipeline that I've created allows you to do English semantic search. So I'm going to go ahead and hit enter and we get these results back. We get a bunch of sentences that were kind of not direct translations of the sentence that we're looking with, which is what Totova would give you, but sentences that are similar and relevant to what we might have to end up writing about. So I'm playing in the park. We were playing in the park. It was fun playing the park. The kids playing the park. They were playing baseball. They were a group of children playing in the park. These are not things that have a lot of overlap Outside of the word playing in part in the text and probably would not be surface if we just did keyword search So that's cool, but what happens if we do a Spanish to English search? So you can see in this block all I'm doing is passing a sentence in Spanish, which is my poorly translated way of Saying I played at the park at the end of the week It doesn't even matter too much that the translation's not that great. Remember we're just learning the language. We're going to filter on language's English and it turns out that we get sentences that are relevant to that query. Even though there is not keyword level overlap. We don't see Spanish words in this English text. We just see the English text that's coming back. We're still getting sentences that are relevant to the sentence that's being passed on Saturdays. We usually visit this park, which is relevant to going to the park at the end of the week. That's really cool and the fact that we can do that without doing much else to the actual pipeline is a Test and Mint to the efficacy of this type of technology Finally, we're going to do English to Spanish search which is the thing that we wanted to enable on the first place So if I go ahead and hit enter we can now complete our assignment We said I went to the park last week in a place sports and we're getting senses about going to the park The last I believe Saturday or Sunday and yesterday I went to the park and some other questions and things that are kind of related to that Again, there's no keyword overlap between these things, right? And there's no translation that's happening. We haven't told our vector database what sentences are translation pairs to the other. All we did was embed them in the same vector space and compare where they are existing relative to one another in vector space and returning the ordered results to that. Now, just to prove to you that this is actually what's happening, I'm going to send in a sentence that is not relevant to what we need but has similar keywords. So these could be the false positives or the incorrect things that should be passing in. and we're going to search over in Spanish. So I say, I need to find a place to park, and the first response that I get back is, where did you park something along those lines? And something about not finding a place to park your spaceship? Right, these are things that are related to parking vehicles, whether they be in space, or whether they be in real life. And that kind of shows us that we are encoding the topics and the concepts that we're interested in finding. So you could take this notebook. You could remove my filter that subsamples it and just scale it up to the entire data set and just see what happens when you do that. You could add in a bunch of other languages, the Totova data set has, telling which translations are crossed, tons and tons of languages, you could just do all of them, just see what pops up. We're gonna clean up our index and delete it here just to make sure we don't have that active anymore and that will be our demo. I'm gonna go ahead and stop showing my screen and go back to the presentation. Alrighty. So we just built semantic search using the same exact pipeline, which was really cool. I want to leave you with some tips and tricks to think about when you're building your own multilingual semantic search applications. Remember to embed your queries and passages differently, especially when you're using PineCon inference. Don't forget about chunking your data, you'll still need to do that. The endpoint doesn't do it for you, and that there's a context when you're about a 507 tokens. Whenever you're doing anything in a cross-lingle fashion, you should create an internal gold standard evaluation set for understanding how well that performance is across languages. It's very, researchers can't possibly test every single cross-lingual language that exists in the world. That would commentatorially, that would be way too many things to test at the same time. So if there's a specific cross-lingual use case that you want to use this model for, you should have a gold validation set of query and the relevant responses that should be pulled up in the top or whatever search you're doing in order to get that back, take advantage of batching in order to embed lots of things at the same time in order to kind of support the rate limiting. And remember that you can use the same pipeline across languages. You don't have to do cross-lingual search. You could just support semantic search in the language with the same language as being the query and the documents. And the fact that you can use the same pipeline across those things is pretty neat. The last thing I would add is that you don't need to see semantic search as a replacement for full-text search. text search, you can use both kinds of search in the same pipeline in order to help users find the things that not only overlap with the words that they're interested in, but also are semantically relevant and similar to the things that they'd like to work on. Alright everybody, that's my presentation. I really hope that you were able to take away something interesting and important from this talk on multilingual semantic search. I did my best to cover a lot of the concepts that you might need in order to understand them. The good thing is that when you work with Python serverless in Panko and Infrains, you You don't need to know too much about how these models are working or what they're doing under the hood. You can just use the end points in order to build your semantic search applications. Personally, I would love to see what everybody is working on and what you're building, what you're thinking about building, so drop that in the chat and I'd love to hear about what types of things you're going to be doing. This is the end of my presentation, and so we're going to open the floor to Q&A. I'll go down the Q&A and just see what people have in mind or what they'd like to ask about. So if there's something that hasn't been addressed, if there's something you'd like me to go over in a little more detail, I can try to do that during this time as well. Thanks everybody, and let's jump in. Cool. It looks like Shrini Vass has said that there is a fashion item recommender that includes pine cone bedding search as well as keyword search. great example of kind of using both of those properties in order to kind of get the thing that we're interested in. The Jinghao said, how do we evaluate semantic search results? This is a great question. This is very specific to the industry that you're coming from or working with. Generally what people do is that they'll have a gold standard dataset of queries that are coming in and you'll have a gold standard of the documents that should be retrieved relative to that query. This is not a trivial dataset to collect. So very often what people might do is implement something, see whether people are finding the relevant thing, if they are also to use that as a relevant way of kind of looking and finding this information. I highly suggest thinking about the type of industry that you're coming from, the types of queries that you're going to be asking, and the types of response that you're going to expect to get, and use that to construct a gold standard data set to evaluate different types of search offerings. Shreenevoss is asking another question about language theory in general. So if it's possible to embed all these languages the same vector space, is that mean, or I, sorry, I misunderstood. But if there's no actual language family, can that still be turned into a vector embedding? Yes, that is correct, because the way that these embedding models work, they don't care about the language that you're actually passing in. So theoretically, you could use multiple languages in the same sentence and it'll get embedded in some sort of vector space. Well, that work as well as if you're using one language inside that search, maybe not. But that will, that's something you can test for sure. And since English is a thing that people actually use, it might exist already on the internet, so it might have been scraped to create these embeddings. So it might actually exist. Sure you've asked us a bunch of questions, so thank you for your enthusiasm here. So how are we deciding what words discard and what words to keep? When tokenization happens for the corpus, there is an optimization algorithm that kind of decides what are the most frequent core occurring, like biograms or trigrams or accommodations of words or concepts, And you kind of turn the ones that are most frequently occurring into the tokens in the vocabulary, so you can kind of reuse that token over and over and over again. There's a little out of scope of kind of what we want to cover. So if you're really interested in tokenization, we have some articles on our website, but there's also a lot of good content about sentence piece and how it works on the internet. So keep an eye out for that. Trinivas, another question here about having tons of different languages. If there's a new language that isn't actually existing in the 88 languages, could that actually work? and what's the approach to kind of support that? This is a great question. If I remember correctly, there is a textbook from Hugging Face that they released a few years ago on Transformers. And I believe one of the problems that they go over in the textbook is out of domain, multi-lingual, unnamed entity recognition. So this is the same idea of like, OK, you don't actually have this language in the corpus. So you don't teach this model to work in this language. But how far can you get by training on a related language family and kind of going from there? So I would argue that hypothetically, There's probably some merit here to language families and whether you can do enough to kind of get close to that language in order to do something that's helpful for there. An example of this might happen in the real world is that there are lots of languages in India, like there's Hindi and I'm going to draw these so there are grajathi languages and grajathi as not as much content on the internet as Hindi does. So how would you build a language model for grajathi specifically? Well, you might have to include some Hindi data in order to kind of take advantage of that language distribution. Okay, Terry, great question. So what is the benefit of doing this over translating directly? So I actually have a slide for you that kind of helps answer this question. So why not do translation? So first of all, translation is actually really expensive. You can either do it automatically, in which case you will still have to verify if the translated vocabulary is correct, or you have to pay some eye to do it, in which case you can get very expensive. And then you have to think about all of the ways that the translation is going to be supported for. And you have to support that over time. So if you update your documentation, you have to retranslate it and make sure the retranslation is actually accurate. Now, full-text search is great. You can do keyword search with that. You can support and create those keywords for those specific languages, the cost of those you're looking for and help them kind of do that. But if you're actually trying to leverage some antique search, so if you're actually trying to help people type inquiries and you're trying to find stuff that's might not be as relevant to the query that you're passing in. But might not have keyword overlap at all, but it's still relevant. You're not going to be able to do that using translation whatsoever because you're still using keyword search under the hood, that's where multilingual semantic search really allows you to kind of survive. So that's the use case. And I encourage you to kind of look into how much it would cost to translate and verify your stuff. It gets very scary really quickly. So doing something like this is a good addition. It might not be a replacement for having translation, especially if you're a really really large website. But it definitely allows you to incorporate semantic search, which is not going to be something you get from translation. So even has a question about being new to vector databases, what's the best practice for indexing, how to create it, how to upload some people saying saving the data and JSON format is good. What do I think? So there are a lot of different ways of embedding and upserting data using pine cone. There isn't really like a, there are some documentation on our website on how you can efficiently index and upsert data. I've shown some examples of how to embed data and then upsert and how you can kind of respect limits in order to do that really nicely. So it really just depends on the type of data that you're working with. I would try to do it programmatically so that you can respect rate limits. We have some scripts on our website that let you do this programmatically so that you can just like copy, paste the scripts, point it at the data that you have located and just do that upcursion in the meeting. Vasey, Leo's, I hope I'm pronouncing that correctly. You have a question about training your own and multilingual embedding model for a specific domain or extending one. Yes, you could totally do that. This is not something that Pinecone offers right now, but the idea is that if you're doing so good at generalizing across languages, then maybe you're still going to be pretty good at the end domain stuff, but your point is great. If you're working on a domain or vocabulary that has a lot of specific words, that might not be tokenized the same as the words that exist across the internet or of their specific languages, then yeah, you could find to in a model to do the same type of learning objective, like I described before, like mask language learning, or whatever, in order to have a better representation of the thing that you're doing. And that is generally the recommendation if your domain is extremely hyper specific. So yeah, it's possible. Or you can just see how well stock embedding models work. And that's where I would start. See how well the embedding models are working. Think about how much lift you want and then try to train toward that. Kind of Jinghao is another question. If you ask some negative questions, like give me some health care companies other in the health field, the model's performance isn't very good. How can you improve this? So lots of different ways to go about this. This is a classic kind of issue that's difficult to handle with semantic search, you could have a layer in between the query and the vector database that transforms the user query into a lot of different other queries that no longer negative that are no longer negatives. Search over those queries, rank those and return them back to the user. So you're basically, you might ask another LLM like, hey, take this input query from a user that has a negative in it or is phrase as if it has a negative question, convert it into a bunch of positive questions and then just search for results for those positive questions. That's one way of doing going to get off the top of my head, but there isn't like a nice stock way of doing this just with the vector database. That's more of a data engineering slash going to production type of problem. Well, this is this is the stuff I love answer it, right? This is like that theoretical like, what is actually happening with these LLM? What is kind of going on? So is language kind of interfacing with this thinking or how does Pine can kind of account for these cultural nuances? So it is important to keep in mind that the limits for the most model and the limits for different languages and nuances are going to be restricted to the model. We're just hosting the model because we find it really useful for people to do. But we haven't augmented the model further in order to make it have a much wider token window for example. We're applying to expand the amount of models that we're hosting and though other models that we might host might have larger token windows which might be able to conduct windows which might be able to handle those use cases. In addition, how do we handle of cultural nuances, for example, the wedding in an Indian context might be really different than the concept of a wedding in a Western context. So how can you account that for that in search? That's a great question. That's something to do needs a kind of benchmark and kind of see if your domain are you actually getting the relevant things or you do need to apply something else like a multilingual re-ranker, which is something that Patanko offers as well, in order to kind of get the more relevant cultural results. Or maybe you need to augment the user's input query in order to add additional context for information to get the things that you're interested in. A new great question. We've answered this kind of earlier. This theoretically, it should support things like English and Spanish because then betting model doesn't care what languages are being passed in because it's been training all these things. If you're a use case involves English or Spanish, you should have a gold standard data set just to make sure that it is doing what it says on the team. I'd love for you to believe me like all as you want, but it's always important to have your own evaluation stuff in-house to understand what's going to happen. Next question from ShriniVos. Thank you so much. This is a great comment from me. I'm glad to you enjoyed it. If anyone has any more questions, please drop them in the chat. I would love to understand them and help you kind of figure things out. We have 10 more minutes. I'm here. Please use me as a resource. Tell me about things that you're building, especially related to a multilingual search. I'd love to help you kind of figure out how to go about all these sorts of things. So, men, nilos is asking what happens when the embedding model is used for a dataset that is a low resource language. So this is the term of art that people use when we talk about languages that don't have as much data. And this is the idea that you might have a language that exists on this side of the distribution where you have way, way less data or you might not even like show up on the graph relative to English or whatever. So the idea is that because you're training across languages in an unsupervised fashion, you're not telling the model what language it's working on. You might be able to pick up information across language families in order to help your model learn about something. So let me give you an example related to the one I use prior. So Hindi is a really big language in India, and so is English. So there are a lot of Hindi speakers and a lot of English speakers. I speak with Gajrathe, which is a language that's relegated to a specific state in India. So there aren't many speakers of Gajrathe outside of India. Outside of the state that the language comes from in India. So there's going to be less language data of that language in the world compared to the language of Hindi and English. But, Grajadhiyan Hindi are mutually intelligible. So I can understand when someone speaks Hindi to me. And if someone only speaks Hindi, I speak Grajadhiyan, they can understand some of the words that I'm using. So there is a language family that kind of exists that kind of branches those two languages. So if I wanted to build a large language model, that could help me learn more Grajadhiy for example, I would probably need to take advantage of the fact that there's a lot of Hindi texts out there and a lot of other Indian languages out there that I could use as training data in addition to the Grigrati subset that I have to kind of learn that additional information. So that's what you would need to do. So theoretically, it would still be fine, even though that data does not exist only for Greek. Like the motivation here is not to build a large language model for specific languages all the time. You wanna build something that kind of learns relationships across languages, which lets you leverage that type of information. Igor, great question. If we use a big popular model like OpenAS, text in Benning 3, will we have better results? Really depends on the task that you're working with, really depends on the thing that you're trying to do. Multi-lingual E5 large is designed for multi-lingual tasks. So it's got an retrieval task. So it's gonna be really, really good at doing that. Open AI text in Benning 3 is kind of trying to do a bunch of different things. Like it's trying to be really good and letting you do classification. It might be really good at letting you do some sort of multi-lingual things. But you might not know the advantage of using an open source model like multi-lingual E5 large is that at least you can look at the researchers' research papers really try to understand what exactly is happening under the hood. But look, if it ain't broke, don't fix it. Like if you're using OpenI, text embedding 3, and it works really well, and it's good for your use cases, you can still use that with PineCon. You can still embed your stuff and just upload it into your vector database and not have to worry about it too much. So that's theoretically you will benefit if you're doing specifically a model-lingual or multilingual semantic search and you want to scale across that, you will benefit from using a model like this, but you should have your gold standard data set and kind of go from there. Jinghao has a good question. If the data in Panko needs to be constantly updated, is there any lower cross approach? I know that our serverless offering allows you to decrease the costs that you have relative to using pods. So you don't have to keep something up all the time and kind of doing that sort of stuff. But I'm not sure if there's a clear way to getting around the updates. I'm happy to refer you to any of our solutions engineers. If you have any questions specifically on your use case, so feel free to reach out to me or to give to kind of help you get started with that. It looks like we're nearing the end of a lot of the questions that are kind of coming in. If there are, we have a few more minutes, so if anyone has any other burning questions that like throw in, I'm happy to kind of help you learn about how to think about these sort of things. One thing I will do is stop sharing my screen and throw in the example into the chat so that everyone can kind of take away that for users. So let me go ahead and give you the link to the example page, which I will drop into the chat. Here you go, this is the specific example. And if you wanna read my article about this concept, you can look at this article on Python, which should help you learn about these sort of things. All right. I think that's pretty much everyone's questions. If anybody has anything else that they're interested in learning about, please feel free to contact me at urgentatpinecone.io, which I will drop in a chat here, or on LinkedIn. I'm happy to help you think about these problems. Or at least get you in touch with the people that should help you figure them out. Gives, what's a you? Yeah, thanks, Arjun. And it looks like you might actually solve two questions left in the Q&A. Ah, sorry, I missed those. Let's see. Rahul, you're asking about querying the correct table when you have a text to SQL model. And when you have multiple table columns and names, I'm not quite sure how you can go about building the text to SQL model without any more context. So feel free to email me for a follow-up there. I can kind of help you think about that. plus as a fun question, what happens if you ask an margin language? I'm not really sure. But that is something you can test with the endpoint. You can try to grab the notebook, type in something, and what you think a margin language would be and see what happens. That would be really fun and exciting. Or you can find a constructed language, like something that people have kind of made and see if that actually ends up working. Thank you. This is my fun level collection that I really enjoy. All right, I think that's everything. Yeah, thanks so much. Yeah, and we will follow up with everyone that attended today with the recording and the resources to the notebook and the blog, the Argent road via email. Shortly after the session, but yeah, thanks again for everyone joining. Thanks everybody and feel free to reach out."