{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Video RAG over Webinars with Pinecone, Anthropic and AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the workshop! In this notebook, we'll setup a simple video RAG workflow using Pinecone, Claude and AWS. We'll take an input set of videos and ingest them (using Claude in pre and post processing) in order to allow for an contextual RAG experience over a traditionally vexing dataset. \n",
    "\n",
    "\n",
    "We'll need to create the following preprocessing pipeline to make it all work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In doing so, we'll convert a multimodal problem into a purely text one on search, and leave the complex multimodal ingestion to the Claude Bedrock API. This saves us time and a bit of complexity on the multimodal embedding front!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies and scripting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important Environmental Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Data as Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin by transcribing and processing our video data.\n",
    "\n",
    "**Don't forget to upload your videos manually into the data folder!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video preprocessing: Transcription and Frames\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from preprocessing.config import data_dir, videos_dir\n",
    "from preprocessing.preprocess_videos import *\n",
    "\n",
    "\n",
    "video_files = os.listdir(videos_dir)\n",
    "print(video_files)\n",
    "# add root dir to video files\n",
    "video_files = [os.path.join(videos_dir, f) for f in video_files]\n",
    "\n",
    "all_videos_data = {}\n",
    "transcriptions_dir = os.path.join(data_dir, \"transcriptions\")\n",
    "frames_and_words_dir = os.path.join(data_dir, \"frames_and_words\")\n",
    "frames_dir = os.path.join(data_dir, \"frames\")\n",
    "# folder setup\n",
    "try:\n",
    "    os.mkdir(transcriptions_dir)\n",
    "    os.mkdir(frames_and_words_dir)\n",
    "    os.mkdir(frames_dir)\n",
    "except FileExistsError:\n",
    "        print(\"Folders already exist. Please delete them to start fresh or ensure they are empty.\")\n",
    "\n",
    "for video_path in video_files:\n",
    "    transcription = transcribe_video(video_path)\n",
    "\n",
    "    video_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        \n",
    "    # Write transcription out as json\n",
    "    transcription_filename = os.path.join(transcriptions_dir, video_filename + \"_transcription.json\")\n",
    "    with open(transcription_filename, \"w\") as f:\n",
    "        json.dump(transcription[\"text\"], f)\n",
    "        \n",
    "    frames = extract_frames(frames_dir, video_path, INTERVAL)\n",
    "    frames_and_words = assign_words_to_frames(transcription, frames)\n",
    "        \n",
    "    frames_and_words_filename = os.path.join(frames_and_words_dir, video_filename + \"_frames_and_words.json\")\n",
    "    with open(frames_and_words_filename, \"w\") as f:\n",
    "        json.dump(frames_and_words, f)\n",
    "        \n",
    "    all_videos_data[video_filename] = {\n",
    "            \"transcription\": transcription_filename,\n",
    "            \"frames_and_words\": frames_and_words_filename\n",
    "}\n",
    "\n",
    "    # Optionally, write all_videos_data to a summary file\n",
    "all_videos_data_path = data_dir / \"all_videos_data.json\"\n",
    "with open(all_videos_data_path, \"w\") as f:\n",
    "    json.dump(all_videos_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Claude on Ingest\n",
    "\n",
    "### Contextual Retrieval Primer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Claude to Annotate data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Transcript Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        binary_data = image_file.read()\n",
    "        base_64_encoded_data = base64.b64encode(binary_data)\n",
    "        base64_string = base_64_encoded_data.decode('utf-8')\n",
    "    return base64_string\n",
    "\n",
    "\n",
    "def format_messages_for_claude(user_query, vdb_response):\n",
    "    \"\"\"\n",
    "    Formats the user's query and the vector database response into a structured message for Claude.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The user's query.\n",
    "        vdb_response (list): The response from the vector database, containing images and text.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of messages formatted for Claude.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": []}]\n",
    "    # add in the first query\n",
    "    new_content = [{\"type\": \"text\", \"text\": \"The user query is: \" + user_query}]\n",
    "    # we alternate between text, image, and text, where we introduce the iamge, then the text, then the next image, and so on.\n",
    "    # we append three messages at a time, one for the image, one for the text, and one for the next image.\n",
    "\n",
    "    for item in vdb_response:\n",
    "        img_b64 = convert_image_to_base64(item[\"metadata\"][\"filepath\"])\n",
    "        new_content.extend([\n",
    "            {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Image: \" + item[\"metadata\"][\"filepath\"],\n",
    "            },\n",
    "            {\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": \"image/png\",\n",
    "                \"data\": img_b64\n",
    "            }\n",
    "            },\n",
    "            {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"Contextual description: \" + item[\"metadata\"][\"contextual_frame_description\"]\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Transcript: \" + item[\"metadata\"][\"transcript\"]\n",
    "            }\n",
    "        ])\n",
    "    #reassign\n",
    "    messages[0][\"content\"] = new_content\n",
    "    return messages\n",
    "\n",
    "def ask_claude_vqa_response(user_query, vdb_response):\n",
    "    \"\"\"\n",
    "    Sends the user's query and the vector database response to Claude and gets a response.\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The user's query.\n",
    "        vdb_response (list): The response from the vector database, containing images and text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The response from Claude.\n",
    "    \"\"\"\n",
    "    client = AnthropicBedrock()\n",
    "    messages = format_messages_for_claude(user_query, vdb_response)\n",
    "    system_prompt = '''\n",
    "\n",
    "You are a friendly assistant helping people interpret their videos at their company.\n",
    "\n",
    "You will recieve frames of these videos, with descriptions of what has happened in the frames, as well as a user query\n",
    "\n",
    "Your job is to ingest the images and text, and respond to the user's query or question based on the context provided.\n",
    "\n",
    "Refer back to the images and text provided to guide the user to the appropriate slide, section, webinar, or talk\n",
    "where the information they are looking for is located.\n",
    "    '''\n",
    "    response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=MAX_TOKENS * 10,\n",
    "            system=system_prompt,\n",
    "            messages=messages\n",
    "        )\n",
    "    return response.content[0].text\n",
    "    \n",
    "\n",
    "\n",
    "def ask_claude(img, text):\n",
    "    # best for one off queries\n",
    "    client = AnthropicBedrock(\n",
    "    aws_region=\"us-east-1\")\n",
    "    if img:\n",
    "        img_b64 = convert_image_to_base64(img)\n",
    "        message = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            messages=[\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"source\": \n",
    "                        {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/png\",\n",
    "                            \"data\": img_b64\n",
    "                        }\n",
    "                    },\n",
    "                    {\"type\": \"text\", \"text\": text}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        )\n",
    "    else:\n",
    "        message = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            messages=[{\"role\": \"user\", \"content\": text}]\n",
    "        )\n",
    "    return message.content[0].text\n",
    "\n",
    "\n",
    "\n",
    "def make_claude_transcript_summary(transcript):\n",
    "    client = AnthropicBedrock(\n",
    "    aws_region=\"us-east-1\")\n",
    "\n",
    "    prompt = \"Summarize the following transcript, being as concise as possible:\"\n",
    "    message = client.messages.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt + \": \" + transcript}],\n",
    "        max_tokens=MAX_TOKENS\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Meta Prompt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contextual_frame_description(frame_caption_index, frame_caption_pairs, transcript_summary, window=60, frame_width=15):\n",
    "    # frame caption pair will have an image, and a transcript. Window is in seconds\n",
    "    client = AnthropicBedrock(\n",
    "    aws_region=\"us-east-1\")\n",
    "\n",
    "    # gather context, look 4 frame widths before and after. Make sure not to go out of bounds if near beginning or end of video.\n",
    "    \n",
    "    surrounding_frames = frame_caption_pairs[max(0, frame_caption_index - 4 * frame_width):frame_caption_index + 1]\n",
    "\n",
    "    current_frame = frame_caption_pairs[frame_caption_index]\n",
    "\n",
    "    # summarize past frames\n",
    "    # removed for now\n",
    "   #past_frames_summary = make_claude_transcript_summary(\" \".join([f[\"words\"] for f in surrounding_frames]))\n",
    "    meta_prompt = f'''\n",
    "\n",
    "    You are watching a video and trying to explain what has happened in the video using a global summary, some recent context, and the transcript of the current frame.\n",
    "\n",
    "    The video has been summarized as follows:\n",
    "    {transcript_summary}\n",
    "\n",
    "    The current frame's transcript is as follows:\n",
    "    {current_frame[\"words\"]}\n",
    "\n",
    "    You also want to provide a description of the current frame based on the context provided.\n",
    "\n",
    "    Please describe this video snippet using the information above in addition to the frame visual. Explain any diagrams or code or important text that appears on screen,\n",
    "    especially if the snippet is of a slide or a code snippet. If there are only people in the frame, focus on the transcript and the context provided to describe what has\n",
    "    been talked about. If a question was asked, and answered, include the question and answer in the description as well.\n",
    "\n",
    "    Description:\n",
    "    '''\n",
    "\n",
    "    rich_summary = ask_claude(img=current_frame[\"frame_path\"], text=meta_prompt)\n",
    "    return rich_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Putting it all Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "with open(\"./data/all_videos_data.json\", \"r\") as f:\n",
    "    all_videos_data = json.load(f)\n",
    "\n",
    "\n",
    "finalized_data = []\n",
    "\n",
    "for video, data in all_videos_data.items():\n",
    "    with open(data[\"transcription\"], \"r\") as f:\n",
    "        transcript = json.load(f)\n",
    "    with open(data[\"frames_and_words\"], \"r\") as f:\n",
    "        frame_caption_pairs = json.load(f)\n",
    "\n",
    "    transcript_summary = make_claude_transcript_summary(transcript=transcript)\n",
    "\n",
    "    print(transcript_summary)\n",
    "\n",
    "    for i, pair in tqdm(enumerate(frame_caption_pairs)):\n",
    "        contextual_frame_description = create_contextual_frame_description(\n",
    "            frame_caption_index = i, \n",
    "            frame_caption_pairs=frame_caption_pairs, \n",
    "            transcript_summary=transcript_summary)\n",
    "    # write out the updated frame caption pairs\n",
    "        new_pair = {\n",
    "            \"frame_path\": pair[\"frame_path\"],\n",
    "            \"words\": pair[\"words\"],\n",
    "            \"timestamp\": pair[\"timestamp\"],\n",
    "            \"transcript_summary\": transcript_summary,\n",
    "            \"contextual_frame_description\": contextual_frame_description\n",
    "        }\n",
    "        finalized_data.append(new_pair)\n",
    "\n",
    "# write out the finalized data\n",
    "with open(\"./data/finalized_data.json\", \"w\") as f:\n",
    "    json.dump(finalized_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pinecone\n",
    "\n",
    "### What is Pinecone?\n",
    "\n",
    "### Using AWS Bedrock: Titan Text Embedding Models\n",
    "\n",
    "### Creating Index\n",
    "\n",
    "### A Note about Metadata\n",
    "\n",
    "### Embedding + Upsertion\n",
    "\n",
    "### And we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import boto3\n",
    "\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name,\n",
    ")\n",
    "\n",
    "\n",
    "# Embedding code\n",
    "def titan_text_embedding(\n",
    "    text: str,  # English only and max input tokens 128\n",
    "    dimension: int = 1024,  # 1,024 (default), 384, 256\n",
    "    model_id: str = \"amazon.titan-embed-text-v2:0\"\n",
    "):\n",
    "    payload_body = {\n",
    "        \"inputText\": text,\n",
    "    }\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=json.dumps(payload_body),\n",
    "        modelId=model_id,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    finish_reason = response_body.get(\"message\")\n",
    "\n",
    "    if finish_reason is not None:\n",
    "        raise Exception(f\"Embeddings generation error: {finish_reason}\")\n",
    "\n",
    "    return response_body\n",
    "\n",
    "\n",
    "# read in as json\n",
    "file_path = './data/finalized_data.json'\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "values_to_embed = [item[\"contextual_frame_description\"] for item in data]\n",
    "ids = [item[\"frame_path\"] for item in data]\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "# For large number of embeddings, take care to respect rate limits\n",
    "for v in tqdm(values_to_embed):\n",
    "    embedding = titan_text_embedding(text=v)\n",
    "    embeddings.append(embedding[\"embedding\"])\n",
    "\n",
    "\n",
    "final_vectors = []\n",
    "# Easy way to assign ids. Be careful of overwriting these \n",
    "ids = [x for x in range(0, len(values_to_embed))]\n",
    "\n",
    "for v, e, id in tqdm(zip(data, embeddings, ids)):\n",
    "    final_vectors.append({\n",
    "            \"id\": str(id),\n",
    "            \"values\": e,\n",
    "            \"metadata\": {\n",
    "                \"transcript\": v[\"words\"],\n",
    "                \"filepath\": v[\"frame_path\"],\n",
    "                \"timestamp_start\": v[\"timestamp\"][0],\n",
    "                \"timestamp_end\": v[\"timestamp\"][1],\n",
    "                \"contextual_frame_description\": v[\"contextual_frame_description\"]\n",
    "            }\n",
    "})\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(final_vectors)\n",
    "print(df.head())\n",
    "index_name=\"enriched-claude-vqa\"\n",
    "# replace this as necessary\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "\n",
    "# create index\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    ) \n",
    "\n",
    "index = pc.Index(index_name)\n",
    "# Handy helper function for upsertion!\n",
    "index.upsert_from_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the RAG workflow\n",
    "\n",
    "### Embed Query with Titan\n",
    "\n",
    "### Get results back with metadata\n",
    "\n",
    "### Read in images, pass to Claude, and generate response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"Find me code samples for learning how to use Pinecone with multilingualism\"\n",
    "\n",
    "query_embedding =  titan_text_embedding(text=query_text)\n",
    "\n",
    "response = index.query(vector=query_embedding[\"embedding\"], top_k=5, include_metadata=True)\n",
    "\n",
    "\n",
    "claude_explanation = ask_claude_vqa_response(query_text, response[\"matches\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries about Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries in transcript but not onscreen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queries onscreen but not in transcript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding and interpreting diagrams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinecone-claude",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
